# ways-py - python package as part of the WAYS (What Aren't You Seeing) project phase 2 (WP2)

[Turing Research Engineering Group Hut23 issue](https://github.com/alan-turing-institute/Hut23/issues/407)

<details>
  <summary>
    WAYS project summary
  </summary>
“As you can see in figure 1…” may well be the most frequently made claim in science. But unlike claims concerning data, statistics, models and algorithms, those relating to visualisations are rarely evaluated or verified. So how can data scientists understand visualisations’ effectiveness and expressiveness? What is the visualisation equivalent of q-q plots, R^2 and K-folds tests?

Designing effective visualisations goes far beyond selecting a graph, scales and a ‘pretty’ style. Effective visualisations must negotiate sensitivities and interactions between visual elements (e.g. encodings, coordinate systems, guides, annotations), data (e.g. characteristics, transformations, partitions), and the discriminator function, which in this case is the perceptual and cognitive systems of humans. Despite their criticality, these methodological and design considerations are rarely surfaced, limiting the value extracted from visualisations. What does figure 1 actually visualise?

The ‘What Aren’t You Seeing’ (WAYS) project addresses 1) what we aren’t seeing in visualisations by 2) revealing the relevant knowledge, theory and practices that we are not seeing at the site of visualisation production. Our final goal is the WAYS package/library in which the properties, outcomes and affordances of visualisation designs are depicted through visualisations; a concept we term ‘Precursor Visualisations’. WAYS then addresses the challenge of generating a productive interplay between everyday visualisation work and the epistemology, practice, communication techniques and evaluation methods that should inform visualisation design at source (Robinson). To achieve this, we propose three work packages (WP1-3).

WP1 – Each advance in the efficiency, speed and scale of data analytics has diminished the value of existing visualisation techniques. Our goal is to provide solutions that can scale to thousands of parameters. Using Precursor Visualisations, layouts options are generated by ‘pinning’ the display to focal parameters given the model structure. We will expand the BackFillz.R package to include optimisation and parameterisation training algorithms such as TensorFlow and MCMC, with functionality spanning both Python and R.

WP2 – Technical fieldwork and ‘show and tell’ rapid ethnographies7 will be conducted to inform WAYS. Based on an R package prototype – GRAPHO – that archives code histories alongside the images outputted to the graphics device, we will carry out detailed, remote user research (including pilots in the Bristol Data Study group and the Jane Golding Institute Visualisation Competition) to study: 1) generative and evaluative activities in everyday visualisation workflows 2) the representation of theory within software use, and 3) the affordances and fates of scientists’ ‘creative hacks’ and preparatory visualisations.

WP3 – The final WP will develop auto-generated Precursor Visualisations for a broad range of evaluative and generative tasks involving maps, networks and time series, and for issues including uncertainty, dimension reduction and missing data. By holding data constant across permutations of visual encodings and scales a Precursor can visualise the conditions that generate depiction ambiguities, visual artefacts or representational invariance, or that hide multiscale, composite patterns. Alternatively, data characteristics can be varied (e.g. subsampling, covariation, noise), whilst holding the design constant, to visualise which patterns, boundaries and features can be detected given the data and that design.
  
</details>
